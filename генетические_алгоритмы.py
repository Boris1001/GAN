# -*- coding: utf-8 -*-
"""Генетические_алгоритмы.ipynb

 Automatically generated by Colaboratory!

Original file is located at
    https://colab.research.google.com/drive/1m5QX40aDk0GKMyH_hdpRlI04-N1iGejS
"""

# Commented out IPython magic to ensure Python compatibility.
#Загружаем библиотеки

import pandas as pd # Пандас
import matplotlib.pyplot as plt # Отрисовка графиков
from tensorflow.keras import utils # Для to_categorical
import numpy as np # Numpy
from tensorflow.keras.optimizers import Adam # Оптимизатор
from tensorflow.keras.models import Sequential, Model # Два варианты моделей
from tensorflow.keras.layers import concatenate,Reshape, Input,Conv2DTranspose, Lambda, Dense, Dropout, BatchNormalization, Flatten, Conv1D, Conv2D, LSTM, MaxPooling1D, Activation, GlobalMaxPooling1D, UpSampling1D#Стандартные слои
from tensorflow.keras.losses import MAE
from google.colab import files # Загрузка файлов
from sklearn.preprocessing import StandardScaler, MinMaxScaler # Нормировщики
from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator # Для генерации выборки временных рядов
import tensorflow.keras.backend as K
import random as random # Импортируем модель randim
import numpy as np # Импортируем библиотеку numpy
import matplotlib.pyplot as plt # Импортируем модуль pyplot библиотеки matplotlib для построения графиков
from PIL import Image # Импортируем одель Image для обработки изображений
import time
from tensorflow.keras.utils import plot_model
# Статический вывод графики (графики отображаются в той ячейке, в которой используется plt.show())
# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

# Считываем данные с помощью pandas
base_data = pd.read_csv('/content/drive/My Drive/Базы/traff.csv',header=None)

base_data.head()

data = base_data.iloc[:,1]

# Выводим параметры одного фала
d = data
print(len(d))         # Сколько есть записей
print(len(d.iloc[0])) # Длинна одной строки данных
print(d.iloc[0])      # Пример первой строки данных

data = np.array(data) #Превращаем в numpy массив
for i in range(len(data)):
  data[i] = float(data[i].replace(',','')) #парсим значения

"""**Загрузка данных**"""

#Формируем параметры загрузки данных
xLen = 60     # Анализируем по 60 прошедшим точкам 
valLen = 300  # Используем 300 записей для проверки
trainLen = data.shape[0]-valLen # Размер тренировочной выборки

# Делим данные на тренировочную и тестовую выборки 
Train,Test = np.reshape(data[:trainLen],(-1,1)), np.reshape(data[trainLen+xLen+2:],(-1,1))

# Масштабируем данные (отдельно для X и Y), чтобы их легче было скормить сетке
Scaler = MinMaxScaler()
Scaler.fit(Train)
Train = Scaler.transform(Train)
Test = Scaler.transform(Test)



# Создаем генератор для обучения
trainDataGen = TimeseriesGenerator(Train, Train,             # в качестве параметров наши выборки
                               length=xLen, sampling_rate=1, # для каждой точки
                               batch_size=20)                # размер batch, который будем скармливать модели

# Создаем аналогичный генератор для валидации при обучении
testDataGen = TimeseriesGenerator(Test, Test,
                               length=xLen, sampling_rate=1,
                               batch_size=20)

print(trainLen)

"""**Функции для отображения результатов**"""

# Функция рассчитываем результаты прогнозирования сети
# В аргументы принимает сеть (currModel) и проверочную выборку
# Выдаёт результаты предсказания predVal
# И правильные ответы в исходной размерности yValUnscaled (какими они были до нормирования)
def getPred(currModel, xVal, yVal, yScaler):
  # Предсказываем ответ сети по проверочной выборке
  # И возвращаем исходны масштаб данных, до нормализации
  predVal = yScaler.inverse_transform(currModel.predict(xVal))
  yValUnscaled = yScaler.inverse_transform(yVal)
  
  return (predVal, yValUnscaled)

# Функция визуализирует графики, что предсказала сеть и какие были правильные ответы
# start - точка с которой начинаем отрисовку графика
# step - длина графика, которую отрисовываем
# channel - какой канал отрисовываем
def showPredict(start, step, channel, predVal, yValUnscaled):
  plt.plot(predVal[start:start+step, channel], 
           label='Прогноз')
  plt.plot(yValUnscaled[start:start+step, channel], 
           label='Базовый ряд')
  plt.xlabel('Время')
  plt.ylabel('Значение Close')
  plt.legend()
  plt.show()
  
# Функция расёта корреляции дух одномерных векторов
def correlate(a, b):
  # Рассчитываем основные показатели
  ma = a.mean() # Среднее значение первого вектора
  mb = b.mean() # Среднее значение второго вектора
  mab = (a*b).mean() # Среднее значение произведения векторов
  sa = a.std() # Среднеквадратичное отклонение первого вектора
  sb = b.std() # Среднеквадратичное отклонение второго вектора
  
  #Рассчитываем корреляцию
  val = 0
  if ((sa>0) & (sb>0)):
    val = (mab-ma*mb)/(sa*sb)
  return val

# Функция рисуем корреляцию прогнозированного сигнала с правильным
# Смещая на различное количество шагов назад
# Для проверки появления эффекта автокорреляции
# channels - по каким каналам отображать корреляцию
# corrSteps - на какое количество шагов смещать сигнал назад для рассчёта корреляции
# showGraf - показываем график или нет
# returnData - возвращаем массивы автокорреляции или нет
def autoCorr(channels, corrSteps, predVal, yValUnscaled, showGraf = True, returnData = False):
  # Проходим по всем каналам
  for ch in channels:
    corr = [] # Создаём пустой лист, в нём будут корреляции при смезении на i рагов обратно
    yLen = yValUnscaled.shape[0] # Запоминаем размер проверочной выборки

      # Постепенно увеличикаем шаг, насколько смещаем сигнал для проверки автокорреляции
    for i in range(corrSteps):
      # Получаем сигнал, смещённый на i шагов назад
      # predVal[i:, ch]
      # Сравниваем его с верными ответами, без смещения назад
      # yValUnscaled[:yLen-i,ch]
      # Рассчитываем их корреляцию и добавляем в лист
      corr.append(correlate(yValUnscaled[:yLen-i,ch], predVal[i:, ch]))

    own_corr = [] # Создаём пустой лист, в нём будут корреляции при смезении на i рагов обратно

      # Постепенно увеличикаем шаг, насколько смещаем сигнал для проверки автокорреляции
    for i in range(corrSteps):
      # Получаем сигнал, смещённый на i шагов назад
      # predVal[i:, ch]
      # Сравниваем его с верными ответами, без смещения назад
      # yValUnscaled[:yLen-i,ch]
      # Рассчитываем их корреляцию и добавляем в лист
      own_corr.append(correlate(yValUnscaled[:yLen-i,ch], yValUnscaled[i:, ch]))

    # Отображаем график коррелций для данного шага
    if showGraf: #Если нужно показать график
      plt.plot(corr, label='предсказание на ' + str(ch+1) + ' шаг')
      plt.plot(own_corr, label='Эталон')

  if showGraf: #Если нужно показать график
    plt.xlabel('Время')
    plt.ylabel('Значение')
    plt.legend()
    plt.show()

  if returnData: #Если нужно вернуть массивы автокорреляции
    return corr, own_corr

"""**Проверочная выборка** **"""

# Создадим генератор проверочной выборки, из которой потом вытащим xVal, yVal для проверки
DataGen = TimeseriesGenerator(Test, Test,
                               length=xLen, sampling_rate=1,
                               batch_size=len(Test)) # Размер batch будет равен длине нашей выборки

xVal = []
yVal = []
for i in DataGen:
  xVal.append(i[0])
  yVal.append(i[1])

xVal = np.array(xVal)
yVal = np.array(yVal)

"""**Сетка**"""

def createConvNet(net):
  makeFirstNormalization = net[0] # Делаем ли нормализацию
  maxPoolKernel = net[1]          # Ядро пулинга
  firstConvSize = 2 ** net[2]     # Размер первого cвёрточного слоя
  firstConvKernel = net[3]        # Ядро первого свёрточного слоя
  activation1 = net[4]            # Функция активации первого слоя

  secondConvProb = net[5]         # Вероятность добавления доп. сверточного слоя
  secondConvcount = net[6]        # Кол-во доп. сверточных слоев
  secondConvSizeMin = net[7]      # Мин. размер доп. свёрточного слоя
  secondConvSizeMax = net[8]      # Макс. размер доп. свёрточного слоя
  secondConvSizeMode = net[9]     # Выбирать размеры слоев случайно: 0 - случайно, 1 - последовательно, 2 - мин., 3 - макс.
  secondConvKernelMin = net[10]   # Мин. ядро доп. свёрточного слоя
  secondConvKernelMax = net[11]   # Макс. ядро доп. свёрточного слоя
  secondConvKernelMode = net[12]  # Выбирать ядро доп. свёрточного слоя: 0 - случайно, 1 - последовательно, 2 - мин., 3 - макс.
  makePool2 = net[13]             # Вероятность применения пуллинга?
  activation2 = net[14]           # Функция активации доп. слоя

  denseProb = net[15]             # Вероятность добаления полносвязного слоя
  denseCount = net[16]            # Кол-во полносвязных слоев
  denseSizeMin = net[17]          # Мин. размер полносвязного слоя
  denseSizeMax = net[18]          # Макс. размер полносвязного слоя
  denseSizeMode = net[19]         # Выбирать размер полносвязного слоя: 0 - случайно, 1 - последовательно, 2 - мин., 3 - макс.
  activation6 = net[20]           # Функция активации полносвязного слоя

  shape = (xLen, 1)               # Размер входных данных
  inputs = Input(shape)           # Входной слой


  # Список активационных функций
  activation_list = ['linear','relu','tanh','softmax','sigmoid']

  choise = random.random()

  if choise < makeFirstNormalization:
    x = BatchNormalization()(inputs)
    x = Conv1D(firstConvSize, firstConvKernel, padding ='same')(x)
    x1 = Activation(activation_list[activation1])(x)
  else:                        
    x = Conv1D(firstConvSize, firstConvKernel, padding ='same')(inputs)
    x = Activation(activation_list[activation1])(x)

  if random.random() < makePool2:    # Добавление пулинга
    x = MaxPooling1D(maxPoolKernel)(x)

  for i in range(secondConvcount):   # Добавление блока  
    if random.random() < secondConvProb:
      size = 0
      kernel = 0

      if secondConvSizeMode == 0:
        size = int(np.random.uniform(secondConvSizeMin, secondConvSizeMax))
      elif secondConvSizeMode == 1:
        if (i < len(range(secondConvSizeMin, secondConvSizeMax))):
          size = range(secondConvSizeMin, secondConvSizeMax)[i]
        else:
          size = secondConvSizeMax
      elif secondConvSizeMode == 2:
        size = secondConvSizeMin
      elif secondConvSizeMode == 3:
        size = secondConvSizeMax

      if secondConvKernelMode == 0:
        kernel = int(np.random.uniform(secondConvKernelMin, secondConvKernelMax))
      elif secondConvKernelMode == 1:
        if (i < len(range(secondConvKernelMin, secondConvKernelMin))):
          kernel = range(secondConvKernelMin, secondConvKernelMin)[i]
        else:
          kernel = secondConvKernelMin
      elif secondConvKernelMode == 2:
        kernel = secondConvKernelMin
      elif secondConvKernelMode == 3:
        kernel = secondConvKernelMin

      x = Conv1D(2 ** size, kernel, padding ='same')(x)
      x = Activation(activation_list[activation2])(x)    

  x = Flatten()(x)
  
  # Добавление полносвязного слоя
  for i in range(denseCount):
    if random.random() < denseProb:
      size = 0

      if denseSizeMode == 0:
        size = int(np.random.uniform(denseSizeMin, denseSizeMax))
      elif denseSizeMode == 1:
        if (i < len(range(denseSizeMin, denseSizeMax))):
          size = range(denseSizeMin, denseSizeMax)[i]
        else:
          size = denseSizeMax
      elif denseSizeMode == 2:
        size = denseSizeMin
      elif denseSizeMode == 3:
        size = denseSizeMax

      x = Dense(2 ** size)(x)
      x = Activation(activation_list[activation6])(x)

  x = Dense(1)(x)

  model = Model(inputs, x) 
  return model

"""**Функция вычисления результата работы сети**"""

def evaluateNet(net, ep, verb):
  val = 0
  time.time()
  model = createConvNet(net) # Создаем модель createConvNet

  # Компилируем модель
  model.compile(optimizer=Adam(lr=1e-4),
                  loss='mse')

  history = model.fit_generator(trainDataGen,
                    epochs=5, 
                    verbose=verb,
                    validation_data=testDataGen)
    
  val = history.history["val_loss"][-1] # Возвращаем точность на проверочной выборке с последней эпохи
  
  return val, model                      # Возвращаем точность

"""**Функция создания списка случайных параметров**"""

def createRandomNet():
  net = []
  net.append(random.randint(0,1))   # Вероятность применения нормализации
  net.append(random.randint(2,3))   # Ядро maxPooling
  net.append(random.randint(2,8))  # Первый свёрточный слой от 8 до 1024 нейронов
  net.append(random.randint(3,9))   # Ядро первого свёрточного слоя от 3 до 9
  net.append(random.randint(0,4))   # Функция активации  

  net.append(random.randint(0,1))   # Вероятность добавления доп. сверточного слоя
  net.append(random.randint(0,8))   # Кол-во доп. сверточных слоев
  net.append(2)                     # Мин. размер сверточного слоя
  net.append(6)                     # Макс. размер сверточного слоя
  net.append(random.randint(0,3))   # Как выбирать размер: случайно, последовательно, мин., макс.
  net.append(2)                     # Мин. размер ядра
  net.append(9)                     # Макс. размер ядра
  net.append(random.randint(0,3))   # Как выбирать ядро: случайно, последовательно, мин., макс.
  net.append(random.randint(0,1))   # Вероятность применения MaxPooling
  net.append(random.randint(0,4))   # Функция активации
  
  net.append(random.randint(0,1))   # Вероятность добавления полносвязного слоя
  net.append(random.randint(0,8))   # Кол-во полносвязных слоев
  net.append(2)                     # Мин. размер полносвязного слоя
  net.append(6)                     # Макс. размер полносвязного слоя
  net.append(random.randint(0,3))   # Как выбирать размер: случайно, последовательно, мин., макс.
  net.append(random.randint(0,4))   # Функция активации
  
  return net

"""**запуск**"""

# Вводные данные
n = 20              # Общее число ботов
nsurv = 10          # Количество выживших (столько лучших переходит в новую популяцию)
nnew = n - nsurv    # Количество новых (столько новых ботов создается)
l = 21              # Размер бота
epohs = 10          # Количество эпох
mut = 0.09          # Коэффициент мутаций
popul = []          # Массив популяции
val = []            # Одномерный массив значений этих ботов

#Создаём ботов
for i in range(n):
  popul.append(createRandomNet())

# Запускаем сетку
for it in range(epohs):                 # Пробегаем по всем эпохам
  val = []                              # Обнуляем значения бота
  curr_time = time.time()
  for i in range(n):                    # Пробегаем в цикле по всем ботам 
    bot = popul[i]                      # Берем очередного бота
    f, model_sum = evaluateNet(bot, 3, 0) # Вычисляем точность текущего бота
    val.append(f)                       # Добавляем полученное значение в список val
  
  sval = sorted(val, reverse=0)         # Сортируем val
  # Выводим 5 лучших ботов
  print(it, time.time() - curr_time, " ", sval[0:5],popul[:5]) 
  
  newpopul = []                         # Создаем пустой список под новую популяцию
  for i in range(nsurv):                # Пробегаем по всем выжившим ботам
    index = val.index(sval[i])          # Получаем индекс очередного бота из списка лучших в списке val
    newpopul.append(popul[index])       # Добавляем в новую популяцию бота из popul с индексом index
    
  for i in range(nnew):                 # Проходимся в цикле nnew-раз  
    indexp1 = random.randint(0,nsurv-1) # Случайный индекс первого родителя в диапазоне от 0 до nsurv - 1
    indexp2 = random.randint(0,nsurv-1) # Случайный индекс первого родителя в диапазоне от 0 до nsurv - 1
    botp1 = newpopul[indexp1]           # Получаем первого бота-родителя по indexp1
    botp2 = newpopul[indexp2]           # Получаем второго бота-родителя по indexp2    
    newbot = []                         # Создаем пустой список под значения нового бота    
    net4Mut = createRandomNet()         # Создаем случайную сеть для мутаций
    for j in range(l):                  # Пробегаем по всей длине размерности (784)      
      x = 0      
      pindex = random.random()          # Получаем случайное число в диапазоне от 0 до 1

      # Если pindex меньше 0.5, то берем значения от первого бота, иначе от второго
      if pindex < 0.5:
        x = botp1[j]
      else:
        x = botp2[j]
      
      # С вероятностью mut устанавливаем значение бота из net4Mut
      if (random.random() < mut):
        x = net4Mut[j]
        
      newbot.append(x)                  # Добавляем очередное значение в нового бота      
    newpopul.append(newbot)             # Добавляем бота в новую популяцию      
  popul = newpopul

model = createConvNet(popul[0])
#model.summary()
plot_model(model, show_layer_names=False, show_shapes=True)

# Обучение сетки

model.compile(optimizer=Adam(lr=1e-4),
                loss='mse')

history = model.fit_generator(trainDataGen,
                    epochs=30, 
                    validation_data=testDataGen)

# отображаем результат

plt.plot(history.history['loss'], 
         label='Средняя абсолютная ошибка на обучающем наборе')
plt.plot(history.history['val_loss'], 
         label='Средняя абсолютная ошибка на проверочном наборе')
plt.ylabel('Средняя ошибка')
plt.legend()
plt.show()

# Прогнозируем данные текущей сетью
currModel = model # Выбираем текущую модель

# Прогнозируем данные
(predVal, yValUnscaled) = getPred(currModel, xVal[0], yVal[0], Scaler) 

# Отображаем графики
showPredict(0, 400, 0, predVal, yValUnscaled)

# Отображаем корреляцию
# Используем выходной канал - только open
# Проверяем корреляцию на глубину 60 шагов
autoCorr([0], 20, predVal, yValUnscaled)

# 
